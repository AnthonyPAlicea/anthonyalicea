---
layout: post.njk
title: "It's Not Context Engineering, It's Context Authoring"
excerpt: Words set proper mental models and expectations.
date: 2025-08-12
tags: ['post','front-page']
---
# {{ title }}

When working with LLMs we shouldn't call it **"context engineering"**, we should call it **"context authoring"**.

Words matter, because they help us set proper mental models and expectations. When it comes to LLM context, *engineering* sets an incorrect mental model, while *authoring* puts you in the right frame of mind.

The two concepts are distinct:

### Engineering
- Building rigid systems following strict protocols  
- Following technical specifications and repeatable processes  
- Solving problems through logical debugging  
- Refining for consistency and error reduction  

### Authoring
- Building structured content guiding interpretation  
- Following principles of hierarchy and clear communication  
- Solving communication challenges by providing examples 
- Refining for clarity and intended outcomes 

Saying you are *engineering* context, like prompts, implies you achieve a predictable outcome through rigorous iteration and testing. That's setting a poor expectation. LLMs aren't programmable systems in the traditional sense, and their output is anything but predictable and repeatable.

*Authoring* is instead about *communication*. You get the best results from LLMs by thinking in terms of **authoring content for comprehensibility and constraints**.

When you write context for an LLM, you're crafting communication that guides its pattern-matching nature to respect your intent, constraints, and desired outcomes â€” the same as writing clear documentation or instructions for a human collaborator.

Giving an LLM a role to play, patterns of example output, background reasoning, specific instructions, and more are **not** engineering. You are triggering pattern matching in a system derived from human communication. 

That includes code, as code is a form of communication. Code is a grammatical structure with the purpose of communicating intent to a human or system. Good code is good communication, because the intent is understood correctly.

Your input into the communication-trained machine, whatever it is, needs to be good communication. And that, in turn, is what the LLM will attempt to infer as output: the structure of good communication (whether correct or not).

On average, the better the communication input, the closer the communication output will adhere to your intent.

The industry may have settled on *engineering* as a term, but I encourage you to think in terms of *authoring*. You'll get the best results from your efforts.